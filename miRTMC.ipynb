{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4867cabf-1090-4c8a-bb66-cf9c82ccf889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, precision_score, recall_score, f1_score,\n",
    "    roc_curve, auc, precision_recall_curve, average_precision_score\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- Config ----------------\n",
    "MIRNA_SIM_F = \"mirna_similarity.npy\"\n",
    "GENE_SIM_F = \"gene_similarity.npy\"\n",
    "INTERACTION_F = \"interaction.npy\"\n",
    "MODEL_WEIGHTS_F = \"mirtmc_final_weights.pt\" # File lưu trọng số mô hình cuối cùng\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# Chú ý: Cần kiểm tra device ID có tồn tại không\n",
    "if USE_CUDA:\n",
    "    try:\n",
    "        # Sử dụng device ID 0\n",
    "        DEVICE = torch.device(\"cuda:0\")\n",
    "    except RuntimeError:\n",
    "        # Fallback về CPU nếu cuda:0 không khả dụng\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Use CUDA:\", USE_CUDA, \"Device:\", DEVICE)\n",
    "\n",
    "TEST_SUBSET = False       # chạy full\n",
    "\n",
    "# ADMM / SVD params  \n",
    "K_SVD = 500\n",
    "OVERSAMPLE = 20\n",
    "N_POWER_ITERS = 2\n",
    "MU = 1e-3\n",
    "RHO = 1.5\n",
    "MAX_ITER = 100\n",
    "TOL = 1e-4\n",
    "\n",
    "# CV params\n",
    "K_FOLD = 5     \n",
    "LAMBDA_CANDIDATES = [0.001, 0.01, 0.1, 1.0, 10.0]  \n",
    "TORCH_DTYPE = torch.float32\n",
    "\n",
    "# CHỈ TÍNH TOP K = 10\n",
    "TOP_K_RANGE = [10]\n",
    "# ----------------------------------------\n",
    " \n",
    "def evaluate_ranking(I_pred, test_pos, m, n):\n",
    "    \"\"\"Tính AUC và AUPR toàn cục.\"\"\"\n",
    "    all_mi_gene_pairs = np.array([[i, j] for i in range(m) for j in range(n)])\n",
    "    labels = np.zeros(len(all_mi_gene_pairs), dtype=int)\n",
    "    test_pos_set = set(map(tuple, test_pos))\n",
    "    \n",
    "    # Chỉ xét các cặp (miRNA, Gene) có trong ma trận kích thước m x n\n",
    "    is_mi_gene_pair = (all_mi_gene_pairs[:, 0] < m) & (all_mi_gene_pairs[:, 1] < n)\n",
    "    valid_pairs = all_mi_gene_pairs[is_mi_gene_pair]\n",
    "\n",
    "    # Cập nhật labels cho các tương tác dương trong tập test\n",
    "    for k, (mi, gene) in enumerate(valid_pairs):\n",
    "        if (mi, gene) in test_pos_set:\n",
    "            labels[k] = 1\n",
    "            \n",
    "    scores = I_pred[valid_pairs[:, 0], valid_pairs[:, 1]]\n",
    "    \n",
    "    auc_val = roc_auc_score(labels, scores)\n",
    "    aupr = average_precision_score(labels, scores)\n",
    " \n",
    "    fpr, tpr, _ = roc_curve(labels, scores)\n",
    "    precision_pr, recall_pr, _ = precision_recall_curve(labels, scores)\n",
    "\n",
    "    return auc_val, aupr, fpr, tpr, labels, scores, precision_pr, recall_pr\n",
    "\n",
    "def calculate_topk_metrics(I_pred, test_pos, m, n, k_range=TOP_K_RANGE):\n",
    "    \"\"\"\n",
    "    Tính Precision@K, Recall@K, F1@K, HR@K, NDCG@K cho K trong k_range.\n",
    "    Trả về một dictionary lồng nhau: {K: {metric_name: value}}\n",
    "    \"\"\"\n",
    "    test_pos_set = set(map(tuple, test_pos))\n",
    "    results_by_k = {k: {'P': [], 'R': [], 'F1': [], 'HR': [], 'NDCG': []} for k in k_range}\n",
    "    \n",
    "    # Lặp qua TẤT CẢ miRNA (mi)\n",
    "    for i in range(m):\n",
    "        scores = I_pred[i, :]\n",
    "        arg_sort_desc = np.argsort(scores)[::-1]\n",
    "        \n",
    "        # 1. Xác định các Gene đích thực sự (relevance) cho miRNA i\n",
    "        relevant_genes_in_test = [j for j in range(n) if (i, j) in test_pos_set]\n",
    "        \n",
    "        # Chỉ tính metrics cho các miRNA có tương tác dương trong tập test\n",
    "        if not relevant_genes_in_test:\n",
    "            continue\n",
    "            \n",
    "        relevance = np.zeros(n, dtype=int)\n",
    "        for j in relevant_genes_in_test:\n",
    "            relevance[j] = 1\n",
    "        \n",
    "        num_true_positives = len(relevant_genes_in_test)\n",
    "\n",
    "        # 2. Lặp qua các ngưỡng K\n",
    "        for k in k_range:\n",
    "            top_k_genes_indices = arg_sort_desc[:k]\n",
    "            \n",
    "            # Tính Hits và Relevance cho Top K\n",
    "            hits = [g_idx for g_idx in top_k_genes_indices if g_idx in relevant_genes_in_test]\n",
    "            num_hits = len(hits)\n",
    "            \n",
    "            # --- Precision, Recall, F1 ---\n",
    "            precision_k = num_hits / k\n",
    "            recall_k = num_hits / num_true_positives\n",
    "            f1_k = 2 * precision_k * recall_k / (precision_k + recall_k) if (precision_k + recall_k) > 0 else 0\n",
    "            \n",
    "            results_by_k[k]['P'].append(precision_k)\n",
    "            results_by_k[k]['R'].append(recall_k)\n",
    "            results_by_k[k]['F1'].append(f1_k)\n",
    "            \n",
    "            # --- HR (Hit Ratio) ---\n",
    "            hr_k = 1 if num_hits >= 1 else 0\n",
    "            results_by_k[k]['HR'].append(hr_k)\n",
    "\n",
    "            # --- NDCG ---\n",
    "            top_k_relevance = np.array([relevance[j] for j in top_k_genes_indices])\n",
    "            \n",
    "            # DCG\n",
    " \n",
    "            dcg = np.sum(top_k_relevance / np.log2(np.arange(2, len(top_k_relevance) + 2)))\n",
    "            \n",
    "            # IDCG\n",
    "            ideal_relevance = np.sort(relevance)[::-1][:k]\n",
    "            idcg = np.sum(ideal_relevance / np.log2(np.arange(2, len(ideal_relevance) + 2)))\n",
    "            \n",
    "            ndcg_val = dcg / idcg if idcg > 0 else 0\n",
    "            results_by_k[k]['NDCG'].append(ndcg_val)\n",
    "\n",
    "    # 3. Tính giá trị trung bình cho tất cả K\n",
    "    final_metrics = {}\n",
    "    for k in k_range:\n",
    "        final_metrics[k] = {\n",
    "            'P': np.mean(results_by_k[k]['P']) if results_by_k[k]['P'] else 0,\n",
    "            'R': np.mean(results_by_k[k]['R']) if results_by_k[k]['R'] else 0,\n",
    "            'F1': np.mean(results_by_k[k]['F1']) if results_by_k[k]['F1'] else 0,\n",
    "            'HR': np.mean(results_by_k[k]['HR']) if results_by_k[k]['HR'] else 0,\n",
    "            'NDCG': np.mean(results_by_k[k]['NDCG']) if results_by_k[k]['NDCG'] else 0,\n",
    "        }\n",
    "        \n",
    "    return final_metrics\n",
    "\n",
    "def run_evaluation_only(X_completed_t, I, positives, m, n, best_lambda):\n",
    " \n",
    "    \n",
    "    aucs = []; auprs = []\n",
    "    all_labels = []; all_scores = []\n",
    "    topk_fold_metrics = {k: {metric: [] for metric in ['P', 'R', 'F1', 'HR', 'NDCG']} for k in TOP_K_RANGE}\n",
    "\n",
    "    I_pred_saved = X_completed_t[:m, m:].cpu().numpy()\n",
    "\n",
    "    print(\"\\nStarting K-Fold Evaluation using Saved Weights...\")\n",
    "    kf_eval = KFold(n_splits=K_FOLD, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(kf_eval.split(positives)):\n",
    "        train_pos, test_pos = positives[train_idx], positives[test_idx]\n",
    "        \n",
    "        auc, aupr, _, _, labels, scores, _, _ = evaluate_ranking(I_pred_saved, test_pos, m, n)\n",
    "        topk_results = calculate_topk_metrics(I_pred_saved, test_pos, m, n, k_range=TOP_K_RANGE)\n",
    "\n",
    "        aucs.append(auc); auprs.append(aupr)\n",
    "        all_labels.append(labels); all_scores.append(scores)\n",
    "        \n",
    "        # Lưu metrics Top K của fold hiện tại\n",
    "        k_disp = TOP_K_RANGE[0]\n",
    "        for k in TOP_K_RANGE:\n",
    "            for metric in ['P', 'R', 'F1', 'HR', 'NDCG']:\n",
    "                topk_fold_metrics[k][metric].append(topk_results[k][metric])\n",
    "        \n",
    "        print(f\"--- Fold {fold+1}/{K_FOLD} --- AUC={auc:.4f}, AUPR={aupr:.4f}, P@{k_disp}={topk_results[k_disp]['P']:.4f}, HR@{k_disp}={topk_results[k_disp]['HR']:.4f}, NDCG@{k_disp}={topk_results[k_disp]['NDCG']:.4f}\")\n",
    "\n",
    "    mean_auc, std_auc = np.mean(aucs), np.std(aucs)\n",
    "    mean_aupr, std_aupr = np.mean(auprs), np.std(auprs)\n",
    "    \n",
    "    all_results_row = {'lambda': best_lambda, 'AUC_mean': mean_auc, 'AUC_std': std_auc, 'AUPR_mean': mean_aupr, 'AUPR_std': std_aupr}\n",
    "    \n",
    "    # Thêm metrics Top K\n",
    "    for k in TOP_K_RANGE:\n",
    "        for metric in ['P', 'R', 'F1', 'HR', 'NDCG']:\n",
    "            mean_val = np.mean(topk_fold_metrics[k][metric])\n",
    "            std_val = np.std(topk_fold_metrics[k][metric])\n",
    "            all_results_row[f'{metric}@{k}_mean'] = mean_val\n",
    "            all_results_row[f'{metric}@{k}_std'] = std_val\n",
    "    \n",
    "    results_df = pd.DataFrame([all_results_row])\n",
    " \n",
    "    final_labels = np.concatenate(all_labels)\n",
    "    final_scores = np.concatenate(all_scores)\n",
    "    \n",
    "    return results_df, best_lambda, final_labels, final_scores\n",
    "\n",
    " \n",
    "def main():\n",
    "    print(\"Loading numpy files...\")\n",
    "    # Tải ma trận tương đồng (S_m, S_g) và tương tác (I)\n",
    "    try:\n",
    "        S_m = np.load(MIRNA_SIM_F)\n",
    "        S_g = np.load(GENE_SIM_F)\n",
    "        I = np.load(INTERACTION_F)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: File not found. Please ensure {MIRNA_SIM_F}, {GENE_SIM_F}, and {INTERACTION_F} are in the same directory.\")\n",
    "        print(f\"Missing file: {e.filename}\")\n",
    "        return\n",
    "        \n",
    "    m, n = I.shape\n",
    "    positives = np.argwhere(I == 1)\n",
    "\n",
    " \n",
    "    if not os.path.exists(MODEL_WEIGHTS_F):\n",
    "         print(f\"Error: Model weights file '{MODEL_WEIGHTS_F}' not found. Cannot run evaluation-only mode.\")\n",
    "         print(\"Please ensure the trained weights file is present or run the full training code first.\")\n",
    "         return\n",
    "\n",
    "    checkpoint = torch.load(MODEL_WEIGHTS_F, map_location=DEVICE)\n",
    "    X_completed_t = checkpoint['X_completed'].to(DEVICE, dtype=TORCH_DTYPE)\n",
    "    best_lambda = checkpoint.get('lambda', 'N/A')\n",
    "    print(f\"Loading final weights from {MODEL_WEIGHTS_F} for evaluation (Best Lambda: {best_lambda})...\")\n",
    "    \n",
    "    # Thực hiện đánh giá K-Fold  \n",
    "    results_df, final_lambda, final_labels, final_scores = run_evaluation_only(\n",
    "        X_completed_t, I, positives, m, n, best_lambda\n",
    "    )\n",
    "    \n",
    "    # Sắp xếp và in bảng kết quả  \n",
    "    print(\"\\n\\nFINAL RESULTS SUMMARY (K=10):\")\n",
    "    \n",
    "    # Lấy hàng của lambda tốt nhất  \n",
    "    best_row = results_df.iloc[0]\n",
    "\n",
    "    print(f\"\\nAll Results (Mean ± Std for Loaded $\\lambda$: {final_lambda}):\\n\")\n",
    "    \n",
    "    summary_data = []\n",
    "    summary_data.append(('Metric', 'Mean ± Std'))\n",
    "    summary_data.append(('---', '---'))\n",
    "    \n",
    "    # Metrics Global\n",
    "    summary_data.append(('AUC', f\"{best_row['AUC_mean']:.4f} ± {best_row['AUC_std']:.4f}\"))\n",
    "    summary_data.append(('AUPR', f\"{best_row['AUPR_mean']:.4f} ± {best_row['AUPR_std']:.4f}\"))\n",
    "    summary_data.append(('---', '---'))\n",
    "    \n",
    "    # Metrics Top K (K=10)\n",
    "    k = 10\n",
    "    if f'P@{k}_mean' in best_row.index:\n",
    "        summary_data.append((f'P@{k}', f\"{best_row[f'P@{k}_mean']:.4f} ± {best_row[f'P@{k}_std']:.4f}\"))\n",
    "        summary_data.append((f'R@{k}', f\"{best_row[f'R@{k}_mean']:.4f} ± {best_row[f'R@{k}_std']:.4f}\"))\n",
    "        summary_data.append((f'F1@{k}', f\"{best_row[f'F1@{k}_mean']:.4f} ± {best_row[f'F1@{k}_std']:.4f}\"))\n",
    "        summary_data.append((f'HR@{k}', f\"{best_row[f'HR@{k}_mean']:.4f} ± {best_row[f'HR@{k}_std']:.4f}\"))\n",
    "        summary_data.append((f'NDCG@{k}', f\"{best_row[f'NDCG@{k}_mean']:.4f} ± {best_row[f'NDCG@{k}_std']:.4f}\"))\n",
    "        \n",
    "    print(pd.DataFrame(summary_data[2:], columns=summary_data[:2]).to_string(index=False))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032a9938-630d-47af-82f3-2b1c997df4fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
